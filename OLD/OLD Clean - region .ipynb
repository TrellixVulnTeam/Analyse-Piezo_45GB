{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8ee71e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "========== Traitement de la région GES ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 370 chroniques piezzo\n",
      "Traitement des données\n",
      "76% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2005-01-27.\n",
      "Date de fin : 2015-08-06.\n",
      "Il reste 186 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 186 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n",
      "========== Traitement de la région ARA ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 200 chroniques piezzo\n",
      "Traitement des données\n",
      "94% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2009-10-29.\n",
      "Date de fin : 2018-07-05.\n",
      "Il reste 111 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 111 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n",
      "========== Traitement de la région PDL ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 149 chroniques piezzo\n",
      "Traitement des données\n",
      "85% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2012-01-19.\n",
      "Date de fin : 2020-02-13.\n",
      "Il reste 93 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 93 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n",
      "========== Traitement de la région PAC ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 119 chroniques piezzo\n",
      "Traitement des données\n",
      "92% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2009-06-18.\n",
      "Date de fin : 2017-07-20.\n",
      "Il reste 37 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 37 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n",
      "========== Traitement de la région BFC ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 109 chroniques piezzo\n",
      "Traitement des données\n",
      "93% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2010-03-18.\n",
      "Date de fin : 2018-05-31.\n",
      "Il reste 76 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 76 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n",
      "========== Traitement de la région NAQ ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 831 chroniques piezzo\n",
      "Traitement des données\n",
      "72% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2011-01-13.\n",
      "Date de fin : 2019-03-14.\n",
      "Il reste 424 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 424 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n",
      "========== Traitement de la région OCC ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 274 chroniques piezzo\n",
      "Traitement des données\n",
      "91% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2009-08-20.\n",
      "Date de fin : 2017-08-24.\n",
      "Il reste 127 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 127 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n",
      "========== Traitement de la région NOR ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 149 chroniques piezzo\n",
      "Traitement des données\n",
      "94% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2009-03-26.\n",
      "Date de fin : 2018-11-29.\n",
      "Il reste 102 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 102 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n",
      "========== Traitement de la région IDF ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 71 chroniques piezzo\n",
      "Traitement des données\n",
      "88% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2010-05-27.\n",
      "Date de fin : 2018-10-04.\n",
      "Il reste 37 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 37 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n",
      "========== Traitement de la région HDF ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 193 chroniques piezzo\n",
      "Traitement des données\n",
      "88% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2009-11-19.\n",
      "Date de fin : 2019-12-05.\n",
      "Il reste 120 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 120 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n",
      "========== Traitement de la région CVL ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 185 chroniques piezzo\n",
      "Traitement des données\n",
      "97% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2001-11-29.\n",
      "Date de fin : 2013-08-15.\n",
      "Il reste 121 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 121 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n",
      "========== Traitement de la région BRE ==========\n",
      "Récupération des codes_bss de la région\n",
      "Connexion réussie à SQLite\n",
      "Connexion SQLite est fermée\n",
      "Récupération des 56 chroniques piezzo\n",
      "Traitement des données\n",
      "96% des chroniques ont été conservées pour plage de 8 ans.\n",
      "Date de début : 2006-02-02.\n",
      "Date de fin : 2017-10-05.\n",
      "Il reste 40 chroniques avec suffisamment de données.\n",
      "Nombre total de Nan : 0\n",
      "Il y a 40 chroniques pour le clustering.\n",
      "dataframe enregistré en csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "# Essential Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "# Algorithms\n",
    "from minisom import MiniSom\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, r2_score\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "\n",
    "# Suppression warnings pour calcul np sur des NaN\n",
    "warnings.filterwarnings(action='ignore', message='All-NaN slice encountered')\n",
    "\n",
    "# Liste des fonctions utilisées\n",
    "\n",
    "# Fonction de requetage auprès de la base de données sqlite\n",
    "def f_requete_sql (requete) :\n",
    "    try:\n",
    "        connexion = sqlite3.connect('../data/liste_piezos.db')\n",
    "        curseur = connexion.cursor()\n",
    "        print(\"Connexion réussie à SQLite\")\n",
    "        curseur.execute(requete)\n",
    "        connexion.commit()\n",
    "        resultat = curseur.fetchall()\n",
    "        curseur.close()\n",
    "        connexion.close()\n",
    "        print(\"Connexion SQLite est fermée\")\n",
    "        return resultat\n",
    "    except sqlite3.Error as error:\n",
    "        print(\"Erreur lors du mis à jour dans la table\", error)\n",
    "\n",
    "# Fonction d'affichage des valeurs manquantes\n",
    "def f_plot_nan (dataframe):\n",
    "    f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,6))\n",
    "    sns.heatmap(dataframe.T.isna(), cmap='Blues', cbar=False)\n",
    "    ax.set_title('Missing Values', fontsize=16)\n",
    "    # Masquage des noms de piezo \n",
    "    ax.yaxis.set_visible(False)\n",
    "    # Formatage de la date pour l'affichage\n",
    "    ax.xaxis.set_ticklabels([pd.to_datetime(value).strftime('%Y') for value in ax.xaxis.get_major_formatter().func.args[0].values()])\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "# Fonction pour standardiser les données\n",
    "def scaleColumns(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(df[col])),columns=[col], index=df.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Liste des régions\n",
    "requete = f\"\"\"\n",
    "        SELECT DISTINCT code_region\n",
    "        FROM code_region\n",
    "        INNER JOIN data_piezo\n",
    "            ON departement = code_dpt\n",
    "        WHERE fichier_piezo IS 1\n",
    "         \"\"\"\n",
    "regions = [region[0] for region in f_requete_sql(requete)]\n",
    "\n",
    "\n",
    "# Traitement pour chaque région\n",
    "for region in regions :\n",
    "\n",
    "    print(f\"========== Traitement de la région {region} ==========\")\n",
    "\n",
    "    print(\"Récupération des codes_bss de la région\")\n",
    "    requete = f\"\"\"\n",
    "            SELECT code_bss\n",
    "            FROM data_piezo\n",
    "            INNER JOIN code_region\n",
    "                ON code_dpt = departement\n",
    "            WHERE code_region IS '{region}'\n",
    "            AND fichier_piezo IS 1\n",
    "             \"\"\"\n",
    "    data = f_requete_sql(requete)\n",
    "    code_bss = []\n",
    "    for code in data:\n",
    "        code_bss.append(code[0]) \n",
    "\n",
    "\n",
    "    print(f\"Récupération des {len(code_bss)} chroniques piezzo\")\n",
    "    # Lecture des données du premier piezo de la liste pour initialiser le dataframe\n",
    "\n",
    "    directory = '../data/piezo/'\n",
    "    data = pd.read_csv(directory+f\"{code_bss[0]}.csv\", sep=\";\",  index_col=0, parse_dates=True)\n",
    "    # ‘epoch’: origin is 1970-01-01\n",
    "    data = data.resample('7D', origin =\"epoch\").mean()    \n",
    "    data.rename(columns={\"piezo\":f\"{code_bss[0]}\"}, inplace=True)\n",
    "    # Prise des données depuis 1980\n",
    "\n",
    "    custom_date_parser = lambda x: datetime.strptime(x, \"%Y-%m-%d\")\n",
    "\n",
    "    for file in code_bss[1:]:\n",
    "        df = pd.read_csv(f\"{directory+file}.csv\", sep=\";\",  index_col=0, parse_dates=True, date_parser=custom_date_parser)\n",
    "        df = df.resample('7D', origin =\"epoch\").mean()\n",
    "        df.rename(columns={\"piezo\":f\"{file}\"}, inplace=True)\n",
    "        data = pd.merge(data,df,left_index=True, right_index=True,how='outer')\n",
    "\n",
    "    print(\"Traitement des données\")\n",
    "\n",
    "\n",
    "    # Ajustement des dates de début et de fin\n",
    "    # La plage de temps minimale est fixée à 8 ans. \n",
    "    # La date de début et de fin correspondent à des dates où un maximum de piezos n'ont pas de valeur manquante.\n",
    "    num_years = 0.0\n",
    "    pourcentage_NaN = 0.0\n",
    "\n",
    "    # Minimum de 8 ans d'intervalle\n",
    "    # Si une date n'est pas calculable, num_years sera égal à NaN et la boucle s'arrête, ce qui n'est pas bon.\n",
    "    # Comme NaN n'est jamais égal à NaN, la condition d'arrêt n'est pas bonne si num_years n'est égal elle même et possède donc une valeur\n",
    "\n",
    "    while num_years < 8 or num_years != num_years :\n",
    "        pourcentage_NaN += 0.01\n",
    "        date_init = data[(data.isnull().sum(axis=1)/data.shape[1] < pourcentage_NaN) == True].index.min()\n",
    "        date_fin = data[(data.isnull().sum(axis=1)/data.shape[1] < pourcentage_NaN) == True].index.max()\n",
    "        num_years = (date_fin - date_init).days / 365.2425\n",
    "\n",
    "    print(f\"{int(100*(1-pourcentage_NaN))}% des chroniques ont été conservées pour plage de 8 ans.\")\n",
    "    print(f\"Date de début : {date_init.strftime('%Y-%m-%d')}.\")\n",
    "    print(f\"Date de fin : {date_fin.strftime('%Y-%m-%d')}.\")\n",
    "\n",
    "    data_week_from_ = data.loc[date_init:date_fin]\n",
    "\n",
    "    # Suppression des valeurs abérantes\n",
    "    # Utilisation de la méthode IQR pour supprimer les valeurs abbérantes.\n",
    "    # - Calcul des quartiles et de l'écart interquartile\n",
    "    # - Suppression des valeurs < Q1 - 1.5*IQR\n",
    "    # - Suppression des valeurs > Q3 + 1.5*IQR\n",
    "\n",
    "    data_wo_outliers = data_week_from_.copy()\n",
    "    for piezo in data_wo_outliers.columns : \n",
    "        q1 = np.nanquantile(data_wo_outliers[f\"{piezo}\"], .25)\n",
    "        q3 = np.nanquantile(data_wo_outliers[f\"{piezo}\"], .75)\n",
    "        IQR = q3-q1\n",
    "        data_wo_outliers.loc[data_wo_outliers[f\"{piezo}\"] < (q1-1.5*IQR), piezo] = np.NaN\n",
    "        data_wo_outliers.loc[data_wo_outliers[f\"{piezo}\"] > (q3+1.5*IQR), piezo] = np.NaN\n",
    "\n",
    "    # Suppression des piezos avec plus de 10 valeurs manquantes consécutives\n",
    "    max_consec_nan = 10\n",
    "    bss_to_drop = []\n",
    "\n",
    "    for piezo in data_wo_outliers :\n",
    "        compteur = 0\n",
    "        for date in data_wo_outliers.index :\n",
    "            if pd.isnull(data_wo_outliers.loc[date,piezo]):\n",
    "                compteur += 1\n",
    "            else : compteur = 0\n",
    "            if compteur == max_consec_nan:\n",
    "                bss_to_drop.append(piezo)\n",
    "                break\n",
    "\n",
    "    data_clean = data_wo_outliers.copy().drop(bss_to_drop, axis=1)\n",
    "    print(f\"Il reste {data_clean.shape[1]} chroniques avec suffisamment de données.\")\n",
    "    \n",
    "    \n",
    "    # Interpolation linéaire pour les données manquantes\n",
    "    data_interpol = data_clean.copy().interpolate('linear')\n",
    "\n",
    "    # Réajustement des dates de début et de fin\n",
    "    debut = []\n",
    "    fin = []\n",
    "    for column in data_interpol.columns:\n",
    "        debut.append(data_interpol[f\"{column}\"].first_valid_index())\n",
    "        fin.append(data_interpol[f\"{column}\"].last_valid_index()) \n",
    "    # date_debut = plus grande date en partant du début où il n'y a plus de Nan. Inversement pour date_fin    \n",
    "    date_debut = max(debut)\n",
    "    date_fin = min(fin)\n",
    "    data_interpol = data_interpol.loc[date_debut:date_fin]\n",
    "    print(f\"Nombre total de Nan : {data_interpol.isna().sum().sum()}\")\n",
    "    print(f\"Il y a {data_interpol.shape[1]} chroniques pour le clustering.\")\n",
    "\n",
    "\n",
    "    # Normalisation StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    data_norm = scaleColumns(data_interpol.copy())\n",
    "\n",
    "\n",
    "\n",
    "    # Sauvegarde du dataframe traité en format csv en vue du clustering\n",
    "    data_norm.to_csv(f\"../Clustering/data/{region}.csv\", sep = \";\")\n",
    "    print(\"dataframe enregistré en csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf5451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.047px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "695.838px",
    "left": "1292.99px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
